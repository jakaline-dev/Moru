seed: 42
name: #
fabric:
  accelerator: auto
  strategy: auto
  devices: auto
  precision: bf16-true
paths:
  base_checkpoint_path: #
  train_data_path: #
trainer:
  grad_accum_steps: 1
  use_xformers: false
  cache_vae_outputs: true
  cache_te_outputs: true
  max_train:
    method: epoch
    value: 10
  clip_skip: 2
  noise_offset: 0.05
  min_snr: 5
optimizer:
  name: AnyPrecisionAdamW
  init_args:
    lr: 0.0005
lr_scheduler:
  name: cosine_with_restarts
  init_args:
    num_cycles: 3
    num_warmup_steps: 50
preprocess:
  max_chunk: 128
  min_chunk: 32
  caption_template:
    - A picture of {name}
dataset:
  random_crop: true
  random_flip: true
  shuffle_tags: false
  caption_dropout: 0.0
dataloader:
  batch_size: 1
  drop_last: true
  pin_memory: true
  num_workers: 1
  persistent_workers: true
logging:
  logger: null
  save:
    every: epoch
    value: 1
  sample:
    every: epoch
    value: 1
    pipeline:
      prompt: #
      negative_prompt: "bad quality, worse quality"
      width: 512
      height: 512
      num_inference_steps: 20
unet_peft:
  - type: LoRA
    lr: 0.0005
    parameters:
      r: 16
      target_modules:
        - proj_in
        - proj_out
        - to_q
        - to_k
        - to_v
        - to_out.0
        - ff.net.0.proj
        - ff.net.2
      lora_alpha: 16
      lora_dropout: 0.0
      bias: none
      rank_pattern: {}
      alpha_pattern: {}
text_encoder_peft:
  - type: LoRA
    lr: 5.0e-05
    parameters:
      r: 16
      target_modules:
        - q_proj
        - k_proj
        - v_proj
        - out_proj
        - fc1
        - fc2
      lora_alpha: 16
      lora_dropout: 0.0
      bias: none
      rank_pattern: {}
      alpha_pattern: {}
